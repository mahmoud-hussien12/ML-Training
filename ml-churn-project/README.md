# Customer Churn Prediction â€” End-to-End ML Pipeline

## ğŸ“Œ Project Overview

Customer churn is one of the most critical problems for subscription-based businesses.  
Acquiring a new customer is significantly more expensive than retaining an existing one.

This project builds an **end-to-end machine learning system** to **predict customer churn** using structured (tabular) data.  
The model outputs a **probability of churn**, enabling CRM systems to proactively:

- offer discounts or promotions
- improve customer support
- resolve service issues early

This helps **reduce churn rate** and **increase overall business profitability**.

---

## ğŸ§  Business Problem

**Goal:**  
Identify customers who are likely to churn so the business can take preventive actions.

**Why churn prediction matters:**
- Churn directly impacts revenue
- Early detection allows targeted retention strategies
- Even small churn reductions can significantly increase profits

---

## ğŸ“Š Dataset

- **Source:** Telco Customer Churn dataset
- **Size:** ~7,000 customers
- **Target:** `Churn`  
  - `1` â†’ customer churned  
  - `0` â†’ customer retained

### Feature Types

- **Categorical features (16):**
```

gender, SeniorCitizen, Partner, Dependents, PhoneService,
MultipleLines, InternetService, OnlineSecurity, OnlineBackup,
DeviceProtection, TechSupport, StreamingTV, StreamingMovies,
Contract, PaperlessBilling, PaymentMethod

```

- **Numerical features:**
```

tenure, MonthlyCharges, TotalCharges

```

The project explicitly separates **categorical and numerical features** to ensure correct preprocessing and avoid data leakage.

---

## ğŸ—ï¸ Project Structure

```

ml-churn-project/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ base.yaml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ loader.py
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â””â”€â”€ build_features.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ pipeline.py
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ inference/
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ logger.py
â”œâ”€â”€ tests/
â”œâ”€â”€ train.py
â”œâ”€â”€ predict.py
â”œâ”€â”€ README.md
â””â”€â”€ MENTORSHIP_STATUS.md

````

---

## âš™ï¸ ML Pipeline Design

This project follows **production-oriented ML engineering practices**:

### 1ï¸âƒ£ Configuration-driven execution
- All paths, parameters, and settings are controlled via YAML config files
- Enables reproducibility and clean experiment tracking

### 2ï¸âƒ£ Data handling
- Raw data loading with validation
- Stratified train/validation/test split to handle class imbalance
- No preprocessing before splitting (prevents data leakage)

### 3ï¸âƒ£ Feature preprocessing
Implemented using `sklearn` pipelines:

- **Numerical features**
  - Median imputation
  - Standard scaling

- **Categorical features**
  - Most-frequent imputation
  - One-hot encoding (`handle_unknown="ignore"`)

### 4ï¸âƒ£ Modeling
- Baseline model: **Logistic Regression**
- Entire preprocessing + model combined in a single `Pipeline`
- Allows easy model swapping (e.g., XGBoost, LightGBM)

---

## ğŸ“ˆ Evaluation Metrics

Evaluation is performed on the **validation set** only.

### Metrics used:
- **ROC-AUC** (primary metric)
- Precision
- Recall

### Business reasoning:
- Recall for churners is important to avoid missing customers who are likely to leave
- Precisionâ€“recall trade-offs are discussed from a business perspective

---

## ğŸš€ How to Run

### 1ï¸âƒ£ Install dependencies
```bash
poetry install
````

### 2ï¸âƒ£ Train the model

```bash
python train.py --config configs/base.yaml
```

### 3ï¸âƒ£ Run inference (example)

```bash
python predict.py --input data/raw/sample.csv
```

---

## ğŸ“¦ Artifacts

After training, the following artifacts are saved:

* trained model
* preprocessing pipeline

These artifacts can be reused for:

* batch predictions
* REST API deployment
* integration with CRM systems

---

## ğŸ” Key Engineering Decisions

* Used **stratified splits** due to class imbalance
* Kept preprocessing inside sklearn pipelines to avoid leakage
* Chose Logistic Regression as a strong, interpretable baseline
* Avoided notebooks for training to simulate real production workflows
* Selected model based on ROC-AUC metric
* Selected top features using feature importance for lightgbm and cofficient for logistic regression

---

## ğŸ”® Future Improvements

If this were extended further:

* Hyperparameter tuning with cross-validation
* Gradient boosting models (XGBoost / LightGBM)
* Threshold optimization based on business costs
* Model monitoring and drift detection
* Deployment as a FastAPI service

---

## ğŸ‘¤ Author

Built by a senior software engineer transitioning into Machine Learning Engineering,
with a focus on **production-quality ML systems**, not just model accuracy.

```
â€œThis project is under active development.â€
